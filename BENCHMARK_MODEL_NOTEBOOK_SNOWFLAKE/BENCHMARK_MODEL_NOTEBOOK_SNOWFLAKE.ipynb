{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "lastEditStatus": {
   "notebookId": "g7bdxuizmdu3tq4nmjyu",
   "authorId": "8838612320674",
   "authorName": "NMMABA2",
   "authorEmail": "nmmaba2@gmail.com",
   "sessionId": "9be901f6-20ef-4557-b512-da613abedd23",
   "lastEditTime": 1771098766815
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92d672a-a1ff-4806-9dce-5f8c9a4f051b",
   "metadata": {
    "codeCollapsed": true,
    "name": "cell1"
   },
   "source": [
    "# Water Quality Prediction: Benchmark Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173e8dca-8e21-478c-b9d8-8162214025ef",
   "metadata": {
    "codeCollapsed": true,
    "name": "cell2",
    "collapsed": false
   },
   "source": [
    "## Challenge Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d38782b-973e-4f63-83b3-3e556abae629",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell3"
   },
   "source": [
    "Welcome to the EY AI & Data Challenge 2026!  \n",
    "The objective of this challenge is to build a robust **machine learning model** capable of predicting water quality across various river locations in South Africa. In addition to accurate predictions, the model should also identify and emphasize the key factors that significantly influence water quality.\n",
    "\n",
    "Participants will be provided with a dataset containing three water quality parameters ‚Äî **Total Alkalinity**, **Electrical Conductance**, and **Dissolved Reactive Phosphorus** ‚Äî collected between 2011 and 2015 from approximately 200 river locations across South Africa. Each data point includes the geographic coordinates (latitude and longitude) of the sampling site, the date of collection, and the corresponding water quality measurements.\n",
    "\n",
    "Using this dataset, participants are expected to build a machine learning model to predict water quality parameters for a separate validation dataset, which includes locations from different regions not present in the training data. The challenge also encourages participants to explore feature importance and provide insights into the factors most strongly associated with variations in water quality.\n",
    "\n",
    "This challenge is designed for participants with varying levels of experience in data science, remote sensing, and environmental analytics. It offers a valuable opportunity to apply machine learning techniques to real-world environmental data and contribute to advancing water quality monitoring using artificial intelligence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea5ca99-0ab8-4117-8bf1-991714e656be",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell4"
   },
   "source": [
    "**About the Notebook:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86be35cd-7ec3-4697-b476-efb378803e53",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell5"
   },
   "source": [
    "In this notebook, we demonstrate a basic workflow that serves as a foundation for the challenge. The model has been developed to predict **water quality parameters** using features derived from the **Landsat** and **TerraClimate** datasets. Specifically, four spectral bands ‚Äî **SWIR22** (Shortwave Infrared 2), **NIR** (Near Infrared), **Green**, and **SWIR16** (Shortwave Infrared 1) ‚Äî were utilized from Landsat, along with derived spectral indices such as **NDMI** (Normalized Difference Moisture Index) and **MNDWI** (Modified Normalized Difference Water Index). In addition, the **PET** (Potential Evapotranspiration) variable was incorporated from the **TerraClimate** dataset to account for climatic influences on water quality.\n",
    "\n",
    "The dataset spans a five-year period from **2011 to 2015**. Using **API-based data extraction** methods, both Landsat and TerraClimate features were retrieved directly from the [Microsoft Planetary Computer portal](https://planetarycomputer.microsoft.com).\n",
    "\n",
    "These combined spectral, index-based, and climatic features were used as predictors in a regression model to estimate three key water quality parameters: **Total Alkalinity (TA)**, **Electrical Conductance (EC)**, and **Dissolved Reactive Phosphorus (DRP)**.\n",
    "\n",
    "Please note that this notebook serves only as a starting point. Several assumptions were made during the data extraction and model development process, which you may find opportunities to improve upon. Participants are encouraged to explore additional features, enhance preprocessing techniques, or experiment with different regression algorithms to optimize predictive performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57447c9d-ceca-4a26-8066-2dca61e0e224",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell6"
   },
   "source": [
    "## Load In Dependencies\n",
    "The following code installs the required Python libraries (found in the requirements.txt file) in the Snowflake environment to allow successful execution of the remaining notebook code. After running this code for the first time, it is required to ‚Äúrestart‚Äù the kernal so the Python libraries are available in the environment. This is done by selecting the ‚ÄúConnected‚Äù menu above the notebook (next to ‚ÄúRun all‚Äù) and selecting the ‚Äúrestart kernal‚Äù link. Subsequent runs of the notebook do not require this ‚Äúrestart‚Äù process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7871c0-d5f3-45da-a289-3d19db67bf15",
   "metadata": {
    "language": "python",
    "name": "cell58"
   },
   "outputs": [],
   "source": "# %pip install uv\n# !uv pip install -r requirements.txt"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53ef74f-52ba-4c63-b412-2f4432408d04",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "# ============================================\n# Setup: Import Libraries and Create Session\n# ============================================\n\n# Snowflake session\nimport snowflake\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data manipulation and analysis\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display\n\n# Multi-dimensional arrays and datasets (e.g., NetCDF, Zarr)\nimport xarray as xr\n\n# Geospatial raster data handling with CRS support\nimport rioxarray as rxr\n\n# Raster operations and spatial windowing\nimport rasterio\nfrom rasterio.windows import Window\n\n# Feature preprocessing and data splitting\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom scipy.spatial import cKDTree\n\n# Machine Learning\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\n# Planetary Computer tools for STAC API access and authentication\n# NOTE: These packages are not available in Snowflake Anaconda\n# import pystac_client\n# import planetary_computer as pc\n# from odc.stac import stac_load\n# from pystac.extensions.eo import EOExtension as eo\n\nfrom datetime import date\nfrom tqdm import tqdm\nimport os\n\nprint(\"‚úÖ All libraries imported successfully!\")\nprint(\"‚úÖ Snowflake session created!\")"
  },
  {
   "cell_type": "markdown",
   "id": "fdfcbb20-8dff-401a-9f55-ed5d08eb6b60",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell9"
   },
   "source": [
    "## Response Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e1bce-f161-4d24-9f6b-a60778c39585",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell10"
   },
   "source": [
    "Before building the model, we first load the **water quality training dataset**. The curated dataset contains samples collected from various monitoring stations across the study region. Each record includes the geographical coordinates (Latitude and Longitude), the sample collection date, and the corresponding **measured values** for the three key water quality parameters ‚Äî **Total Alkalinity (TA)**, **Electrical Conductance (EC)**, and **Dissolved Reactive Phosphorus (DRP)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "35dc01e7-446c-4cec-a2e6-24e63417947f",
   "metadata": {
    "language": "python",
    "name": "cell59"
   },
   "outputs": [],
   "source": "# ============================================\n# Set Database and Schema Context\n# ============================================\nprint(\"=\" * 80)\nprint(\"üîß SETTING SNOWFLAKE CONTEXT\")\nprint(\"=\" * 80)\n\n# Set the database and schema\nsession.use_database(\"EY_WATER_QUALITY\")\nsession.use_schema(\"CHALLENGE\")\n\nprint(\"\\n‚úÖ Context set:\")\nprint(f\"   Database: EY_WATER_QUALITY\")\nprint(f\"   Schema: CHALLENGE\")\nprint(\"=\" * 80)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892acc46-4840-489a-8c69-ecf4123d2a31",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "# ============================================\n# Load all training datasets from Snowflake tables\n# ============================================\nprint(\"\\nüìä Loading data from Snowflake tables...\")\n\n# Ensure we're in the correct database and schema\nsession.use_database(\"EY_WATER_QUALITY\")\nsession.use_schema(\"CHALLENGE\")\n\n# Water quality training data (target variables)\nWater_Quality_df = session.table(\"WATER_QUALITY_TRAINING\").to_pandas()\nprint(f\"‚úÖ Water Quality Training: {Water_Quality_df.shape[0]:,} rows √ó {Water_Quality_df.shape[1]} columns\")\n\n# Landsat features (satellite data)\nLandsat_Features_df = session.table(\"LANDSAT_FEATURES_TRAINING\").to_pandas()\nprint(f\"‚úÖ Landsat Features Training: {Landsat_Features_df.shape[0]:,} rows √ó {Landsat_Features_df.shape[1]} columns\")\n\n# TerraClimate features (climate data)\nTerraClimate_Features_df = session.table(\"TERRACLIMATE_FEATURES_TRAINING\").to_pandas()\nprint(f\"‚úÖ TerraClimate Features Training: {TerraClimate_Features_df.shape[0]:,} rows √ó {TerraClimate_Features_df.shape[1]} columns\")\n\n# Validation datasets\nLandsat_Features_val_df = session.table(\"LANDSAT_FEATURES_VALIDATION\").to_pandas()\nprint(f\"‚úÖ Landsat Features Validation: {Landsat_Features_val_df.shape[0]:,} rows √ó {Landsat_Features_val_df.shape[1]} columns\")\n\nTerraClimate_Features_val_df = session.table(\"TERRACLIMATE_FEATURES_VALIDATION\").to_pandas()\nprint(f\"‚úÖ TerraClimate Features Validation: {TerraClimate_Features_val_df.shape[0]:,} rows √ó {TerraClimate_Features_val_df.shape[1]} columns\")\n\nprint(\"\\n‚ú® All datasets loaded successfully!\")"
  },
  {
   "cell_type": "markdown",
   "id": "1ac87b43-019d-4aa4-bb4b-62b40cc6cda2",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell12"
   },
   "source": [
    "## Predictor Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459bbd80-887c-4c1f-83f4-3d31ffc40551",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell13"
   },
   "source": [
    "Now that we have our water quality dataset, the next step is to gather the predictor variables from the **Landsat** and **TerraClimate** datasets. In this notebook, we demonstrate how to **load previously extracted satellite and climate data** from separate files, rather than performing the extraction directly, which allows for a smoother and faster experience. Participants can refer to the dedicated extraction notebooks‚Äîone for Landsat and another for TerraClimate‚Äîto understand how the data was retrieved and processed, and they can also generate their own output CSV files if needed. Using these pre-extracted CSV files, this notebook focuses on loading the predictor features and running the subsequent analysis and model training efficiently.\n",
    "\n",
    "For more detailed guidance on the original data extraction process, you can review the Landsat and TerraClimate example notebooks available on the Planetary Computer portal:\n",
    "\n",
    "- [Landsat-c2-l2 - Example-Notebook](https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2#Example-Notebook)  \n",
    "- [Terraclimate - Example-Notebook](https://planetarycomputer.microsoft.com/dataset/terraclimate#Example-Notebook)\n",
    "\n",
    "We have used selected spectral bands ‚Äî **SWIR22** (Shortwave Infrared 2), **NIR** (Near Infrared), **Green**, and **SWIR16** (Shortwave Infrared 1) ‚Äî and computed key spectral indices such as **NDMI** (Normalized Difference Moisture Index) and **MNDWI** (Modified Normalized Difference Water Index). These features capture surface moisture, vegetation, and water content characteristics that influence water quality variability.\n",
    "\n",
    "In addition to Landsat features, we also incorporated the **Potential Evapotranspiration (PET)** variable from the **TerraClimate** dataset, which provides high-resolution global climate data. The PET feature captures the atmospheric demand for moisture, representing climatic conditions such as temperature, humidity, and radiation that influence surface water evaporation and thus affect water quality parameters.\n",
    "\n",
    "The predictor features include:\n",
    "\n",
    "- **SWIR22** ‚Äì Sensitive to surface moisture and turbidity variations in water bodies.  \n",
    "- **NIR** ‚Äì Helps in identifying vegetation and suspended matter in water.  \n",
    "- **Green** ‚Äì Useful for detecting water color and surface reflectance changes.  \n",
    "- **SWIR16** ‚Äì Provides information on surface dryness and sediment concentration.  \n",
    "- **NDMI** ‚Äì Derived from NIR and SWIR16, indicates moisture and vegetation‚Äìwater interaction.  \n",
    "- **MNDWI** ‚Äì Derived from Green and SWIR22, effective for distinguishing open water areas and reducing built-up noise.  \n",
    "- **PET** ‚Äì Extracted from the TerraClimate dataset, represents potential evapotranspiration influencing hydrological and water quality dynamics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cced81f1-3b77-4a69-ac48-9d02cc12c647",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell14"
   },
   "source": [
    "### **Tip 1**\n",
    "\n",
    "Participants are encouraged to experiment with different combinations of **Landsat** bands or even include data from other public satellite data sources. By creating mathematical combinations of bands, you can derive various spectral indices that capture surface and environmental characteristics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a0c7e8-9471-4a09-9ac4-6a5b5f0703bd",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell15"
   },
   "source": [
    "### Loading Pre-Extracted Landsat Data\n",
    "\n",
    "In this notebook, we **load previously extracted Landsat data** from CSV files generated in a separate extraction notebook. This approach ensures a smoother and faster workflow, allowing participants to focus on data analysis and model development without waiting for time-consuming data retrieval.\n",
    "\n",
    "Participants are expected to generate their own data extraction CSV files by running the dedicated Landsat extraction notebook. These CSV files can then be used here to smoothly run this benchmark notebook. Participants can refer to the extraction notebook to understand the API-based process, including how individual bands and indices like **NDMI** were computed. Using these pre-extracted CSV files simplifies preprocessing and is ideal for large-scale environmental and water quality analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe34327-f165-452c-98ef-3df67b6ed550",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell16"
   },
   "source": "### **Tip 2**\n\nIn the data extraction process (performed in the dedicated extraction notebooks), a 100 m focal buffer was applied around each sampling location rather than using a single point. Participants may explore creating different focal buffers around the locations (e.g., 50 m, 150 m, etc.) during extraction. For example, if a 50 m buffer was used for ‚ÄúBand 2‚Äù, the extracted CSV values would reflect the average of Band 2 within 50 meters of each location. This approach can help reduce errors associated with spatial autocorrelation.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c250c49-d6ed-42f7-ad90-b2f619de0027",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "# ============================================\n# Loading Pre-Extracted Landsat Data\n# ============================================\n\n\n# Load Landsat features from Snowflake table (already pre-extracted)\nlandsat_train_features = session.table(\"LANDSAT_FEATURES_TRAINING\").to_pandas()\n\nprint(f\"\\n‚úÖ Landsat training features loaded: {landsat_train_features.shape[0]:,} rows √ó {landsat_train_features.shape[1]} columns\")\n\nprint(\"\\nüìã Landsat feature columns:\")\nprint(landsat_train_features.columns.tolist())\n\nprint(\"\\nSample data (first 5 rows):\")\ndisplay(landsat_train_features.head(5))\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"The predictor features used in this benchmark are:\")\nprint(\"=\" * 80)\n\nprint(\"\\n  ‚Ä¢ SWIR22 - Sensitive to surface moisture and turbidity\")\nprint(\"  ‚Ä¢ NIR - Identifies vegetation and suspended matter\")\nprint(\"  ‚Ä¢ Green - Detects water color and surface reflectance\")\nprint(\"  ‚Ä¢ SWIR16 - Surface dryness and sediment concentration\")\nprint(\"  ‚Ä¢ NDMI - Normalized Difference Moisture Index (NIR & SWIR16)\")\nprint(\"  ‚Ä¢ MNDWI - Modified Normalized Difference Water Index (Green & SWIR22)\")\nprint(\"  ‚Ä¢ PET - Potential Evapotranspiration from TerraClimate\")\n\nprint(\"\\n\" + \"=\" * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61676d8d-7141-4682-9687-850e56e3ef28",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": "# ============================================\n# Data Type Conversion for NDMI and MNDWI\n# ============================================\nprint(\"=\" * 80)\nprint(\"üîß DATA TYPE CONVERSION\")\nprint(\"=\" * 80)\n\nprint(\"\\nConverting NDMI and MNDWI columns to float data type to ensure proper\")\nprint(\"numerical operations during model training.\")\n\n# Check if columns exist before conversion\nif 'NDMI' in landsat_train_features.columns:\n    print(\"\\n‚úì Converting NDMI to float...\")\n    landsat_train_features['NDMI'] = landsat_train_features['NDMI'].astype(float)\n    print(f\"  NDMI dtype: {landsat_train_features['NDMI'].dtype}\")\nelse:\n    print(\"\\n‚ö†Ô∏è  NDMI column not found in dataset\")\n\nif 'MNDWI' in landsat_train_features.columns:\n    print(\"\\n‚úì Converting MNDWI to float...\")\n    landsat_train_features['MNDWI'] = landsat_train_features['MNDWI'].astype(float)\n    print(f\"  MNDWI dtype: {landsat_train_features['MNDWI'].dtype}\")\nelse:\n    print(\"\\n‚ö†Ô∏è  MNDWI column not found in dataset\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ Data type conversion complete!\")\nprint(\"=\" * 80)\n\n# Display data types of all columns\nprint(\"\\nüìã All column data types:\")\nprint(landsat_train_features.dtypes)\n\nprint(\"\\nüìä Updated dataset info:\")\nprint(f\"Shape: {landsat_train_features.shape[0]:,} rows √ó {landsat_train_features.shape[1]} columns\")\n\n# Show sample after conversion\nprint(\"\\nSample data after conversion (first 5 rows):\")\ndisplay(landsat_train_features.head(5))"
  },
  {
   "cell_type": "markdown",
   "id": "8ce0ed5d-32c0-48f9-863c-ef84bad110d2",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell18"
   },
   "source": [
    "### Loading Pre-Extracted TerraClimate Data\n",
    "\n",
    "In this notebook, we **load previously extracted TerraClimate data** from CSV files generated in a dedicated extraction notebook. This approach ensures a smoother and faster workflow, allowing participants to focus on data analysis and model development without waiting for time-consuming data retrieval.\n",
    "\n",
    "Participants are expected to generate their own data extraction CSV files by running the dedicated TerraClimate extraction notebook. These CSV files can then be used here to smoothly run this benchmark notebook. Participants can refer to the extraction notebook to understand the API-based process, including how climate variables such as **Potential Evapotranspiration (PET)** were extracted. Using these pre-extracted CSV files ensures consistent, automated retrieval of high-resolution climate data that can be easily integrated with satellite-derived features for comprehensive environmental and hydrological analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862bd48-8134-477a-bcf5-314ac04b2048",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": "# ============================================\n# Loading Pre-Extracted TerraClimate Data\n# ============================================\n\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Load TerraClimate features from Snowflake table (already pre-extracted)\nTerraclimate_df = session.table(\"TERRACLIMATE_FEATURES_TRAINING\").to_pandas()\n\nprint(f\"\\n TerraClimate training features loaded: {Terraclimate_df.shape[0]:,} rows √ó {Terraclimate_df.shape[1]} columns\")\n\nprint(\"\\n TerraClimate feature columns:\")\nprint(Terraclimate_df.columns.tolist())\n\nprint(\"\\nSample data (first 5 rows):\")\ndisplay(Terraclimate_df.head(5))\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Key TerraClimate Feature:\")\nprint(\"=\" * 80)\n\nprint(\"\\n  ‚Ä¢ PET (Potential Evapotranspiration) - Represents the atmospheric demand for\")\nprint(\"    moisture, capturing climatic conditions such as temperature, humidity, and\")\nprint(\"    radiation that influence surface water evaporation and affect water quality\")\nprint(\"    parameters.\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\" TerraClimate data loading complete!\")\nprint(\"=\" * 80)\n\n# Display summary statistics\nprint(\"\\n TerraClimate Data Summary:\")\ndisplay(Terraclimate_df.describe())"
  },
  {
   "cell_type": "markdown",
   "id": "475de826-a08c-4ea1-bbfc-5e9a3240ab22",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell20"
   },
   "source": [
    "## Joining the Predictor Variables and Response Variables\n",
    "\n",
    "Now that we have extracted our predictor variables, we need to join them with the response variables. We use the **combine_two_datasets** function to merge the predictor variables and response variables. The **concat** function from pandas is particularly useful for this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b42801e-e7b9-4b17-b4e4-f65fd971decc",
   "metadata": {
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": "# ============================================\n# Helper Function: Combine Two Datasets\n# ============================================\n\n\n# Combine two datasets vertically (along columns) using pandas concat function\ndef combine_two_datasets(dataset1, dataset2, dataset3):\n    \"\"\"\n    Returns a vertically concatenated dataset.\n    Attributes:\n        dataset1 - Dataset 1 to be combined\n        dataset2 - Dataset 2 to be combined\n        dataset3 - Dataset 3 to be combined\n    \"\"\"\n    data = pd.concat([dataset1, dataset2, dataset3], axis=1)\n    data = data.loc[:, ~data.columns.duplicated()]\n    return data\n\nprint(\"‚úÖ Helper function 'combine_two_datasets' defined successfully!\")\nprint(\"\\nThis function will:\")\nprint(\"  ‚Ä¢ Concatenate datasets horizontally (column-wise)\")\nprint(\"  ‚Ä¢ Remove any duplicate columns\")\nprint(\"  ‚Ä¢ Return a single unified dataset\")\n\nprint(\"\\n\" + \"=\" * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c75dffde-a8e3-4a6e-bbab-bf0d8aac7bfe",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": "print(\"=\" * 80)\nprint(\"üîÄ JOINING PREDICTOR AND RESPONSE VARIABLES\")\nprint(\"=\" * 80)\n\nprint(\"\\nCombining ground data (water quality) and predictor data (Landsat + TerraClimate)\")\nprint(\"into a single dataset for model training.\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Combining ground data and final data into a single dataset\nwq_data = combine_two_datasets(Water_Quality_df, landsat_train_features, Terraclimate_df)\n\nprint(f\"\\n‚úÖ Combined dataset created: {wq_data.shape[0]:,} rows √ó {wq_data.shape[1]} columns\")\n\nprint(\"\\nüìã All columns in combined dataset:\")\nfor i, col in enumerate(wq_data.columns, 1):\n    print(f\"  {i}. {col}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Sample of combined dataset (first 5 rows):\")\nprint(\"=\" * 80)\ndisplay(wq_data.head(5))\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ Dataset joining complete!\")\nprint(\"=\" * 80)\n\nprint(\"\\nThe combined dataset now contains:\")\nprint(\"  ‚Ä¢ Water Quality measurements (TA, EC, DRP) - Response variables\")\nprint(\"  ‚Ä¢ Landsat features (SWIR22, NIR, Green, SWIR16, NDMI, MNDWI) - Predictor variables\")\nprint(\"  ‚Ä¢ TerraClimate features (PET) - Predictor variable\")\nprint(\"  ‚Ä¢ Geographical coordinates (Latitude, Longitude)\")\nprint(\"  ‚Ä¢ Sample dates\")\n\nprint(f\"\\nThis unified dataset is ready for:\")\nprint(\"  ‚Ä¢ Feature engineering\")\nprint(\"  ‚Ä¢ Train/test split\")\nprint(\"  ‚Ä¢ Model training\")\n\nprint(\"\\n\" + \"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "id": "c4573656-100c-49e7-b3bb-536e0a6290ac",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell23"
   },
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "Before model training, missing values in the dataset were carefully handled to ensure data consistency and prevent model bias. Numerical columns were imputed using their median values, maintaining the overall data distribution while minimizing the impact of outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de7372e4-26c6-4bd8-9d7a-39bc35b01a14",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": "# ============================================\n# Handling Missing Values\n# ============================================\nprint(\"=\" * 80)\nprint(\"üîß HANDLING MISSING VALUES\")\nprint(\"=\" * 80)\n\nprint(\"\\nBefore model training, missing values in the dataset were carefully handled to\")\nprint(\"ensure data consistency and prevent model bias. Numerical columns were imputed\")\nprint(\"using their median values, maintaining the overall data distribution while\")\nprint(\"minimizing the impact of outliers.\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Check for missing values before imputation\nprint(\"\\nüìä Missing values BEFORE imputation:\")\nmissing_before = wq_data.isna().sum()\nmissing_before_df = pd.DataFrame({\n    'Column': missing_before.index,\n    'Missing Count': missing_before.values,\n    'Missing %': (missing_before.values / len(wq_data) * 100).round(2)\n})\nmissing_before_df = missing_before_df[missing_before_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n\nif len(missing_before_df) > 0:\n    print(f\"\\n‚ö†Ô∏è  Found {len(missing_before_df)} columns with missing values:\")\n    display(missing_before_df)\nelse:\n    print(\"\\n‚úÖ No missing values found!\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Check data types before imputation\nprint(\"\\nüîç Checking data types...\")\nprint(\"\\nData types of key columns:\")\nfor col in ['NDMI', 'MNDWI', 'NIR', 'GREEN', 'SWIR16', 'SWIR22', 'PET']:\n    if col in wq_data.columns:\n        print(f\"  {col}: {wq_data[col].dtype}\")\n\n# Convert problematic columns to numeric (coerce errors to NaN)\nprint(\"\\nüîÑ Converting columns to numeric type...\")\nnumeric_columns = ['NDMI', 'MNDWI', 'NIR', 'GREEN', 'SWIR16', 'SWIR22', 'PET',\n                  'TOTAL_ALKALINITY', 'ELECTRICAL_CONDUCTANCE', 'DISSOLVED_REACTIVE_PHOSPHORUS']\n\nfor col in numeric_columns:\n    if col in wq_data.columns:\n        wq_data[col] = pd.to_numeric(wq_data[col], errors='coerce')\n        print(f\"  ‚úì Converted {col} to numeric\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Now impute missing values with median for numerical columns\nprint(\"\\nüîÑ Imputing missing values using median...\")\nwq_data = wq_data.fillna(wq_data.median(numeric_only=True))\n\nprint(\"‚úÖ Imputation complete!\")\n\n# Verify missing values after imputation\nprint(\"\\n\" + \"=\" * 80)\nprint(\"\\nüìä Missing values AFTER imputation:\")\nmissing_after = wq_data.isna().sum()\n\nif missing_after.sum() == 0:\n    print(\"\\n‚úÖ No missing values remaining - Dataset is clean!\")\nelse:\n    remaining = missing_after[missing_after > 0]\n    print(f\"\\n‚ö†Ô∏è  Warning: {missing_after.sum()} missing values still remain:\")\n    display(pd.DataFrame({\n        'Column': remaining.index,\n        'Missing Count': remaining.values\n    }))\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Final Data Types:\")\nprint(\"=\" * 80)\nfor col in numeric_columns:\n    if col in wq_data.columns:\n        print(f\"  {col}: {wq_data[col].dtype}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Dataset Summary:\")\nprint(\"=\" * 80)\nprint(f\"Shape: {wq_data.shape[0]:,} rows √ó {wq_data.shape[1]} columns\")\nprint(f\"Total missing values: {wq_data.isna().sum().sum()}\")\n\nprint(\"\\n‚úÖ Data is now ready for train/test split and model training!\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "id": "560a997d-ec88-4ae0-b540-44cd3e5f6cf2",
   "metadata": {
    "codeCollapsed": true,
    "name": "cell25",
    "collapsed": false
   },
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9f7c65-b3a1-405b-b912-cef4e5157bce",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell26"
   },
   "source": [
    "Now let us select the columns required for our model-building exercise. We will consider only **SWIR22**, **NDMI**, and **MNDWI** from the Landsat data, and **PET** from the TerraClimate dataset as our predictor variables. It does not make sense to use latitude and longitude as predictor variables, as they do not have any direct impact on predicting the water quality parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35381365-adf1-4702-a99d-5cec95812ff9",
   "metadata": {
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": "# ============================================\n# MODEL BUILDING - FEATURE SELECTION\n# ============================================\n\n# ============================================\n# Step 1: Check if wq_data exists, if not recreate it\n# ============================================\nprint(\"\\nüîç Checking if wq_data exists...\")\n\ntry:\n    print(f\"‚úì wq_data found: {wq_data.shape}\")\nexcept NameError:\n    print(\"‚ö†Ô∏è  wq_data not found. Recreating from Snowflake tables...\")\n    \n    # Load all three datasets from Snowflake\n    print(\"\\nüìä Loading datasets from Snowflake...\")\n    \n    Water_Quality_df = session.table(\"WATER_QUALITY_TRAINING\").to_pandas()\n    print(f\"  ‚úì Water Quality: {Water_Quality_df.shape}\")\n    \n    landsat_train_features = session.table(\"LANDSAT_FEATURES_TRAINING\").to_pandas()\n    print(f\"  ‚úì Landsat Features: {landsat_train_features.shape}\")\n    \n    Terraclimate_df = session.table(\"TERRACLIMATE_FEATURES_TRAINING\").to_pandas()\n    print(f\"  ‚úì TerraClimate Features: {Terraclimate_df.shape}\")\n    \n    # Combine datasets\n    print(\"\\nüîó Combining datasets...\")\n    wq_data = pd.concat([Water_Quality_df, landsat_train_features, Terraclimate_df], axis=1)\n    wq_data = wq_data.loc[:, ~wq_data.columns.duplicated()]\n    \n    # Fill missing values\n    print(\"üîß Handling missing values...\")\n    numeric_columns = wq_data.select_dtypes(include=[np.number]).columns\n    for col in numeric_columns:\n        wq_data[col] = pd.to_numeric(wq_data[col], errors='coerce')\n    wq_data = wq_data.fillna(wq_data.median(numeric_only=True))\n    \n    print(f\"‚úÖ wq_data created: {wq_data.shape}\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# ============================================\n# Step 2: Show available columns\n# ============================================\nprint(\"\\nüìã Available columns in wq_data:\")\nprint(wq_data.columns.tolist())\n\nprint(\"\\n\" + \"=\" * 80)\n\n# ============================================\n# Step 3: Select required columns\n# ============================================\nprint(\"\\nüéØ Selecting predictor and target variables...\")\n\nrequired_cols = ['SWIR22', 'NDMI', 'MNDWI', 'PET', \n                'TOTAL_ALKALINITY', 'ELECTRICAL_CONDUCTANCE', \n                'DISSOLVED_REACTIVE_PHOSPHORUS']\n\n# Check which columns are available\navailable_cols = [col for col in required_cols if col in wq_data.columns]\nmissing_cols = [col for col in required_cols if col not in wq_data.columns]\n\nif missing_cols:\n    print(f\"\\n‚ö†Ô∏è  Warning: Missing columns: {missing_cols}\")\n    print(\"\\nüîç Searching for similar column names...\")\n    for missing in missing_cols:\n        similar = [c for c in wq_data.columns \n                  if missing.lower() in c.lower() or c.lower() in missing.lower()]\n        if similar:\n            print(f\"  {missing} ‚Üí Found similar: {similar}\")\n            # Use the first similar column found\n            if similar[0] not in available_cols:\n                available_cols.append(similar[0])\n                print(f\"    ‚úì Using '{similar[0]}' instead\")\n\n# Select only available columns\nwq_data = wq_data[available_cols]\n\nprint(f\"\\n‚úÖ Feature selection complete!\")\nprint(f\"\\nFinal dataset shape: {wq_data.shape[0]:,} rows √ó {wq_data.shape[1]} columns\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# ============================================\n# Step 4: Categorize columns\n# ============================================\nprint(\"\\nüìã Selected columns:\")\n\npredictor_cols = [col for col in wq_data.columns \n                 if col not in ['TOTAL_ALKALINITY', 'ELECTRICAL_CONDUCTANCE', 'DISSOLVED_REACTIVE_PHOSPHORUS']]\n\ntarget_cols = [col for col in wq_data.columns \n              if col in ['TOTAL_ALKALINITY', 'ELECTRICAL_CONDUCTANCE', 'DISSOLVED_REACTIVE_PHOSPHORUS']]\n\nprint(\"\\nüéØ Predictor Variables (Features):\")\nfor i, col in enumerate(predictor_cols, 1):\n    print(f\"  {i}. {col}\")\n\nprint(\"\\nüéØ Target Variables (Response):\")\nfor i, col in enumerate(target_cols, 1):\n    print(f\"  {i}. {col}\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# ============================================\n# Step 5: Display sample and summary\n# ============================================\nprint(\"\\nSample of selected dataset (first 5 rows):\")\nprint(\"=\" * 80)\ndisplay(wq_data.head())\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Summary statistics:\")\nprint(\"=\" * 80)\ndisplay(wq_data.describe())\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ DATA IS READY FOR TRAIN/TEST SPLIT AND MODEL TRAINING!\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "id": "f56f994c-0b88-4d19-8426-3fb73c71e2b5",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell28"
   },
   "source": [
    "### **Tip 3**\n",
    "\n",
    "We are developing individual models for each water quality parameter using a common set of features: **SWIR22**, **NDMI**, **MNDWI**, and **PET**. However, participants are encouraged to experiment with different feature combinations to build more robust machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e241de9-35e2-4fa6-9bba-2d5456af838a",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell29"
   },
   "source": [
    "## Helper Functions\n",
    "\n",
    "### Train and Test Split\n",
    "We will now split the data into 70% training data and 30% test data. Scikit-learn (sklearn) is a robust library for machine learning in Python. The `model_selection` module in scikit-learn provides the `train_test_split` function, which can be used for this purpose.\n",
    "\n",
    "### Feature Scaling\n",
    "Before initiating model training, we may need to perform various data preprocessing steps. Here, we demonstrate scaling of the variables **SWIR22**, **NDMI**, **MNDWI**, and **PET** using StandardScaler.\n",
    "\n",
    "Feature scaling is an essential preprocessing step for numerical features. Many machine learning algorithms‚Äîsuch as gradient descent methods, KNN, and linear or logistic regression‚Äîrequire scaling to achieve optimal performance. Scikit-learn provides several scaling utilities. In this notebook, we use **StandardScaler**, which transforms the data so that each feature has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "### Model Training\n",
    "Now that we have the data in a format suitable for machine learning, we can begin training our models. In this demonstration notebook, we build three separate regression models‚Äîone for each target water quality parameter: **Total Alkalinity**, **Electrical Conductance**, and **Dissolved Reactive Phosphorus**. Each model is trained independently to capture the unique relationships between the satellite-derived features and each water quality parameter.\n",
    "\n",
    "We use the **Random Forest Regressor** from the scikit-learn library for model training. Scikit-learn offers a wide range of regression algorithms, along with powerful parameter tuning and customization options.\n",
    "\n",
    "For model training, the predictor variables (e.g., SWIR22, NDMI, MNDWI, and PET) are stored in an array `X`, and the response variable (one of the water quality parameters) is stored in an array `Y`. Note that the response variable should not be included in `X`. Also, latitude, longitude, and sample date are excluded from the predictor variables since they serve only as spatial and temporal references.\n",
    "\n",
    "### Model Evaluation\n",
    "After training the models for the three water quality parameters, the next step is to evaluate their performance. Each regression model‚ÄîTotal Alkalinity, Electrical Conductance, and Dissolved Reactive Phosphorus‚Äîis assessed using:\n",
    "\n",
    "- **R¬≤ Score**: Measures how well the model explains the variance in the observed values.  \n",
    "- **RMSE (Root Mean Square Error)**: Quantifies the average magnitude of prediction errors.\n",
    "\n",
    "Together, these metrics help determine how effectively each model captures variations in water quality across locations and sampling dates. Scikit-learn provides built-in functions to compute both metrics. Participants may also explore additional evaluation techniques or custom metrics to enhance model assessment.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "83aaf34e-0969-4eff-bdbd-c78a0a74aac4",
   "metadata": {
    "language": "python",
    "name": "cell60"
   },
   "outputs": [],
   "source": "# ============================================\n# Helper Functions\n# ============================================\nprint(\"=\" * 80)\nprint(\"üîß DEFINING HELPER FUNCTIONS\")\nprint(\"=\" * 80)\n\nprint(\"\\nWe will now split the data into 70% training data and 30% test data.\")\nprint(\"Scikit-learn (sklearn) is a robust library for machine learning in Python.\")\nprint(\"The scikit-learn library has a model_selection module in which there is a\")\nprint(\"splitting function train_test_split. You can use the same.\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Function 1: Split data into training and test sets\ndef split_data(X, y, test_size=0.3, random_state=42):\n    \"\"\"\n    Split data into training and test sets.\n    \"\"\"\n    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n\nprint(\"‚úÖ Function 1: split_data() - Splits data into 70% train, 30% test\")\n\n# Function 2: Scale features using StandardScaler\ndef scale_data(X_train, X_test):\n    \"\"\"\n    Scale features using StandardScaler.\n    Transforms data so each feature has mean of 0 and standard deviation of 1.\n    \"\"\"\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled, scaler\n\nprint(\"‚úÖ Function 2: scale_data() - Scales features using StandardScaler\")\n\n# Function 3: Train a Random Forest model\ndef train_model(X_train_scaled, y_train):\n    \"\"\"\n    Train a Random Forest Regressor model.\n    \"\"\"\n    model = RandomForestRegressor(n_estimators=100, random_state=42)\n    model.fit(X_train_scaled, y_train)\n    return model\n\nprint(\"‚úÖ Function 3: train_model() - Trains Random Forest Regressor\")\n\n# Function 4: Evaluate model performance\ndef evaluate_model(model, X_scaled, y_true, dataset_name=\"Test\"):\n    \"\"\"\n    Evaluate model and return predictions, R¬≤ score, and RMSE.\n    \"\"\"\n    y_pred = model.predict(X_scaled)\n    r2 = r2_score(y_true, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    \n    print(f\"\\n{dataset_name} Evaluation:\")\n    print(f\"  R¬≤: {r2:.3f}\")\n    print(f\"  RMSE: {rmse:.3f}\")\n    \n    return y_pred, r2, rmse\n\nprint(\"‚úÖ Function 4: evaluate_model() - Evaluates model with R¬≤ and RMSE\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ All helper functions defined successfully!\")\nprint(\"=\" * 80)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ad8e06d5-10a6-419a-b096-52c6f0566e1f",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell30"
   },
   "source": [
    "### **Tip 4**\n",
    "\n",
    "There are many data preprocessing methods available that may help improve model performance. Participants are encouraged to explore various preprocessing techniques as well as different machine learning algorithms to build a more robust model.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3298984c-93ea-49de-8f8e-517935e1d13b",
   "metadata": {
    "language": "python",
    "name": "cell61"
   },
   "outputs": [],
   "source": "# ============================================\n# Train and Test Split\n# ============================================\nprint(\"=\" * 80)\nprint(\"üìä TRAIN AND TEST SPLIT\")\nprint(\"=\" * 80)\n\nprint(\"\\nWe will now split the data into 70% training data and 30% test data.\")\nprint(\"Scikit-learn alias 'sklearn' is a robust library for machine learning in Python.\")\nprint(\"The scikit-learn library has a model_selection module in which there is a\")\nprint(\"splitting function train_test_split.\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Define predictor variables (X) and target variables (y)\npredictor_cols = ['SWIR22', 'NDMI', 'MNDWI', 'PET']\ntarget_cols = ['TOTAL_ALKALINITY', 'ELECTRICAL_CONDUCTANCE', 'DISSOLVED_REACTIVE_PHOSPHORUS']\n\n# Check if columns exist\navailable_predictors = [col for col in predictor_cols if col in wq_data.columns]\navailable_targets = [col for col in target_cols if col in wq_data.columns]\n\nprint(\"\\nüéØ Predictor Variables (X):\")\nfor col in available_predictors:\n    print(f\"  ‚Ä¢ {col}\")\n\nprint(\"\\nüéØ Target Variables (y):\")\nfor col in available_targets:\n    print(f\"  ‚Ä¢ {col}\")\n\n# Extract features (X) and targets (y)\nX = wq_data[available_predictors]\ny_ta = wq_data['TOTAL_ALKALINITY'] if 'TOTAL_ALKALINITY' in wq_data.columns else None\ny_ec = wq_data['ELECTRICAL_CONDUCTANCE'] if 'ELECTRICAL_CONDUCTANCE' in wq_data.columns else None\ny_drp = wq_data['DISSOLVED_REACTIVE_PHOSPHORUS'] if 'DISSOLVED_REACTIVE_PHOSPHORUS' in wq_data.columns else None\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Split data for each target variable\nprint(\"\\nüîÄ Splitting data (70% train, 30% test)...\")\n\n# Total Alkalinity (TA)\nX_train_ta, X_test_ta, y_train_ta, y_test_ta = split_data(X, y_ta, test_size=0.3, random_state=42)\nprint(f\"\\n‚úÖ Total Alkalinity:\")\nprint(f\"   Training set: {X_train_ta.shape[0]} samples\")\nprint(f\"   Test set: {X_test_ta.shape[0]} samples\")\n\n# Electrical Conductance (EC)\nX_train_ec, X_test_ec, y_train_ec, y_test_ec = split_data(X, y_ec, test_size=0.3, random_state=42)\nprint(f\"\\n‚úÖ Electrical Conductance:\")\nprint(f\"   Training set: {X_train_ec.shape[0]} samples\")\nprint(f\"   Test set: {X_test_ec.shape[0]} samples\")\n\n# Dissolved Reactive Phosphorus (DRP)\nX_train_drp, X_test_drp, y_train_drp, y_test_drp = split_data(X, y_drp, test_size=0.3, random_state=42)\nprint(f\"\\n‚úÖ Dissolved Reactive Phosphorus:\")\nprint(f\"   Training set: {X_train_drp.shape[0]} samples\")\nprint(f\"   Test set: {X_test_drp.shape[0]} samples\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ Data split complete!\")\nprint(\"=\" * 80)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7e68f6e-1679-467f-803c-e819d654bbab",
   "metadata": {
    "language": "python",
    "name": "cell31"
   },
   "outputs": [],
   "source": "# ============================================\n# Feature Scaling (Tip 4)\n# ============================================\nprint(\"=\" * 80)\nprint(\"üí° TIP 4: FEATURE SCALING\")\nprint(\"=\" * 80)\n\nprint(\"\\nBefore initiating model training, we may need to perform various data\")\nprint(\"preprocessing steps. Here, we demonstrate scaling of the variables SWIR22,\")\nprint(\"NDMI, MNDWI, and PET using StandardScaler.\")\n\nprint(\"\\nFeature scaling is an essential preprocessing step for numerical features.\")\nprint(\"Many machine learning algorithms‚Äîsuch as gradient descent methods, KNN, and\")\nprint(\"linear or logistic regression‚Äîrequire scaling to achieve optimal performance.\")\nprint(\"Scikit-learn provides several scaling utilities. In this notebook, we use\")\nprint(\"StandardScaler, which transforms the data so that each feature has a mean of\")\nprint(\"0 and a standard deviation of 1.\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Scale features for Total Alkalinity model\nprint(\"\\nüîß Scaling features for Total Alkalinity model...\")\nX_train_ta_scaled, X_test_ta_scaled, scaler_ta = scale_data(X_train_ta, X_test_ta)\nprint(f\"‚úÖ TA features scaled\")\n\n# Scale features for Electrical Conductance model\nprint(\"\\nüîß Scaling features for Electrical Conductance model...\")\nX_train_ec_scaled, X_test_ec_scaled, scaler_ec = scale_data(X_train_ec, X_test_ec)\nprint(f\"‚úÖ EC features scaled\")\n\n# Scale features for Dissolved Reactive Phosphorus model\nprint(\"\\nüîß Scaling features for Dissolved Reactive Phosphorus model...\")\nX_train_drp_scaled, X_test_drp_scaled, scaler_drp = scale_data(X_train_drp, X_test_drp)\nprint(f\"‚úÖ DRP features scaled\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ All features scaled successfully!\")\nprint(\"=\" * 80)\n\nprint(\"\\nüìä Feature scaling complete. Each feature now has:\")\nprint(\"  ‚Ä¢ Mean = 0\")\nprint(\"  ‚Ä¢ Standard Deviation = 1\")"
  },
  {
   "cell_type": "code",
   "id": "d0a855c6-2308-4fed-b3eb-fec460a07a84",
   "metadata": {
    "language": "python",
    "name": "cell62"
   },
   "outputs": [],
   "source": "# ============================================\n# Model Training (Tip 3)\n# ============================================\nprint(\"=\" * 80)\nprint(\"ü§ñ MODEL TRAINING\")\nprint(\"=\" * 80)\n\nprint(\"\\nüí° TIP 3: INDIVIDUAL MODELS FOR EACH PARAMETER\")\nprint(\"=\" * 80)\n\nprint(\"\\nNow that we have the data in a format suitable for machine learning, we can\")\nprint(\"begin training our models. In this demonstration notebook, we build three\")\nprint(\"separate regression models‚Äîone for each target water quality parameter:\")\nprint(\"  ‚Ä¢ Total Alkalinity\")\nprint(\"  ‚Ä¢ Electrical Conductance\")\nprint(\"  ‚Ä¢ Dissolved Reactive Phosphorus\")\n\nprint(\"\\nEach model is trained independently to capture the unique relationships between\")\nprint(\"the satellite-derived features and each water quality parameter.\")\n\nprint(\"\\nWe use the Random Forest Regressor from the scikit-learn library for model\")\nprint(\"training. Scikit-learn offers a wide range of regression algorithms, along with\")\nprint(\"powerful parameter tuning and customization options.\")\n\nprint(\"\\nFor model training, the predictor variables (e.g., SWIR22, NDMI, MNDWI, and PET)\")\nprint(\"are stored in an array X, and the response variable (one of the water quality\")\nprint(\"parameters) is stored in an array y. Note that the response variable should not\")\nprint(\"be included in X. Also, latitude, longitude, and sample date are excluded from\")\nprint(\"the predictor variables since they serve only as spatial and temporal references.\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Train Model 1: Total Alkalinity\nprint(\"\\nüéØ Training Model 1: Total Alkalinity (TA)\")\nprint(\"-\" * 80)\nmodel_ta = train_model(X_train_ta_scaled, y_train_ta)\nprint(f\"‚úÖ Total Alkalinity model trained with {model_ta.n_estimators} trees\")\n\n# Train Model 2: Electrical Conductance\nprint(\"\\nüéØ Training Model 2: Electrical Conductance (EC)\")\nprint(\"-\" * 80)\nmodel_ec = train_model(X_train_ec_scaled, y_train_ec)\nprint(f\"‚úÖ Electrical Conductance model trained with {model_ec.n_estimators} trees\")\n\n# Train Model 3: Dissolved Reactive Phosphorus\nprint(\"\\nüéØ Training Model 3: Dissolved Reactive Phosphorus (DRP)\")\nprint(\"-\" * 80)\nmodel_drp = train_model(X_train_drp_scaled, y_train_drp)\nprint(f\"‚úÖ Dissolved Reactive Phosphorus model trained with {model_drp.n_estimators} trees\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ ALL THREE MODELS TRAINED SUCCESSFULLY!\")\nprint(\"=\" * 80)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce649dda-e996-4f1a-a446-ee22a2a9f12e",
   "metadata": {
    "language": "python",
    "name": "cell63"
   },
   "outputs": [],
   "source": "# ============================================\n# Model Evaluation\n# ============================================\nprint(\"=\" * 80)\nprint(\"üìà MODEL EVALUATION\")\nprint(\"=\" * 80)\n\nprint(\"\\nAfter training the models for the three water quality parameters, the next step\")\nprint(\"is to evaluate their performance. Each regression model‚ÄîTotal Alkalinity,\")\nprint(\"Electrical Conductance, and Dissolved Reactive Phosphorus‚Äîis assessed using:\")\n\nprint(\"\\n  ‚Ä¢ R¬≤ Score: Measures how well the model explains the variance in the observed values.\")\nprint(\"  ‚Ä¢ RMSE (Root Mean Square Error): Quantifies the average magnitude of prediction errors.\")\n\nprint(\"\\nTogether, these metrics help determine how effectively each model captures\")\nprint(\"variations in water quality across locations and sampling dates. Scikit-learn\")\nprint(\"provides built-in functions to compute both metrics. Participants may also\")\nprint(\"explore additional evaluation techniques or custom metrics to enhance model\")\nprint(\"assessment.\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Evaluate Model 1: Total Alkalinity\nprint(\"\\nüéØ MODEL 1: TOTAL ALKALINITY (TA)\")\nprint(\"=\" * 80)\ny_pred_ta, r2_ta, rmse_ta = evaluate_model(model_ta, X_test_ta_scaled, y_test_ta, \"Total Alkalinity Test\")\n\n# Evaluate Model 2: Electrical Conductance\nprint(\"\\nüéØ MODEL 2: ELECTRICAL CONDUCTANCE (EC)\")\nprint(\"=\" * 80)\ny_pred_ec, r2_ec, rmse_ec = evaluate_model(model_ec, X_test_ec_scaled, y_test_ec, \"Electrical Conductance Test\")\n\n# Evaluate Model 3: Dissolved Reactive Phosphorus\nprint(\"\\nüéØ MODEL 3: DISSOLVED REACTIVE PHOSPHORUS (DRP)\")\nprint(\"=\" * 80)\ny_pred_drp, r2_drp, rmse_drp = evaluate_model(model_drp, X_test_drp_scaled, y_test_drp, \"Dissolved Reactive Phosphorus Test\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"üìä SUMMARY OF MODEL PERFORMANCE\")\nprint(\"=\" * 80)\n\nsummary_df = pd.DataFrame({\n    'Model': ['Total Alkalinity', 'Electrical Conductance', 'Dissolved Reactive Phosphorus'],\n    'R¬≤ Score': [r2_ta, r2_ec, r2_drp],\n    'RMSE': [rmse_ta, rmse_ec, rmse_drp]\n})\n\ndisplay(summary_df)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ MODEL EVALUATION COMPLETE!\")\nprint(\"=\" * 80)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "39cec372-283e-404a-a989-714969c53b0d",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell32"
   },
   "source": [
    "## Model Workflow (Pipeline)\n",
    "\n",
    "The complete model development process follows a structured pipeline to ensure consistency, reproducibility, and clarity. Each stage in the workflow is modularized into independent functions that can be reused for different water quality parameters. This modular approach streamlines the process and makes the workflow easily adaptable to new datasets or parameters in the future.\n",
    "\n",
    "The pipeline automates the sequence of steps ‚Äî from data preparation to evaluation ‚Äî for each target parameter. The same set of predictor variables is used, while the response variable changes for each of the three targets: *Total Alkalinity (TA)*, *Electrical Conductance (EC)*, and *Dissolved Reactive Phosphorus (DRP)*. By maintaining a consistent framework, comparisons across models remain fair and interpretable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40808f99-8014-4746-91ae-5b64c61e04a7",
   "metadata": {
    "language": "python",
    "name": "cell33"
   },
   "outputs": [],
   "source": "# ============================================\n# Model Workflow (Pipeline)\n# ============================================\nprint(\"=\" * 80)\nprint(\"üîÑ MODEL WORKFLOW PIPELINE\")\nprint(\"=\" * 80)\n\nprint(\"\\nThe complete model development process follows a structured pipeline to ensure\")\nprint(\"consistency, reproducibility, and clarity. Each stage in the workflow is\")\nprint(\"modularized into independent functions that can be reused for different water\")\nprint(\"quality parameters. This modular approach streamlines the process and makes the\")\nprint(\"workflow easily adaptable to new datasets or parameters in the future.\")\n\nprint(\"\\nThe pipeline automates the sequence of steps ‚Äî from data preparation to\")\nprint(\"evaluation ‚Äî for each target parameter. The same set of predictor variables is\")\nprint(\"used, while the response variable changes for each of the three targets:\")\nprint(\"  ‚Ä¢ Total Alkalinity (TA)\")\nprint(\"  ‚Ä¢ Electrical Conductance (EC)\")\nprint(\"  ‚Ä¢ Dissolved Reactive Phosphorus (DRP)\")\n\nprint(\"\\nBy maintaining a consistent framework, comparisons across models remain fair\")\nprint(\"and interpretable.\")\n\nprint(\"\\n\" + \"=\" * 80)\n\ndef run_pipeline(X, y, param_name=\"Parameter\"):\n    \"\"\"\n    Complete machine learning pipeline for a single water quality parameter.\n    \n    Steps:\n    1. Split data into train/test (70/30)\n    2. Scale features using StandardScaler\n    3. Train Random Forest model\n    4. Evaluate on training set (in-sample)\n    5. Evaluate on test set (out-sample)\n    6. Return results as DataFrame\n    \"\"\"\n    print(f\"\\n{'=' * 60}\")\n    print(f\"Training Model for {param_name}\")\n    print(f\"{'=' * 60}\")\n    \n    # Split data\n    X_train, X_test, y_train, y_test = split_data(X, y)\n    \n    # Scale\n    X_train_scaled, X_test_scaled, scaler = scale_data(X_train, X_test)\n    \n    # Train\n    model = train_model(X_train_scaled, y_train)\n    \n    # Evaluate (in-sample)\n    y_train_pred, r2_train, rmse_train = evaluate_model(model, X_train_scaled, y_train, \"Train\")\n    \n    # Evaluate (out-sample)\n    y_test_pred, r2_test, rmse_test = evaluate_model(model, X_test_scaled, y_test, \"Test\")\n    \n    # Return summary\n    results = {\n        \"Parameter\": param_name,\n        \"R2_Train\": r2_train,\n        \"RMSE_Train\": rmse_train,\n        \"R2_Test\": r2_test,\n        \"RMSE_Test\": rmse_test\n    }\n    \n    return model, scaler, pd.DataFrame([results])\n\nprint(\"\\n‚úÖ Pipeline function defined successfully!\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "code",
   "id": "0b093eac-a455-47c4-b180-4ec30a9f720b",
   "metadata": {
    "language": "python",
    "name": "cell64"
   },
   "outputs": [],
   "source": "# ============================================\n# Run Pipeline for All Water Quality Parameters\n# ============================================\nprint(\"=\" * 80)\nprint(\"üöÄ RUNNING PIPELINE FOR ALL PARAMETERS\")\nprint(\"=\" * 80)\n\n# Define features (X) and targets (y)\nX = wq_data[['SWIR22', 'NDMI', 'MNDWI', 'PET']]\ny_ta = wq_data['TOTAL_ALKALINITY']\ny_ec = wq_data['ELECTRICAL_CONDUCTANCE']\ny_drp = wq_data['DISSOLVED_REACTIVE_PHOSPHORUS']\n\n# Run pipeline for Total Alkalinity\nmodel_ta, scaler_ta, results_ta = run_pipeline(X, y_ta, param_name=\"Total Alkalinity\")\n\n# Run pipeline for Electrical Conductance\nmodel_ec, scaler_ec, results_ec = run_pipeline(X, y_ec, param_name=\"Electrical Conductance\")\n\n# Run pipeline for Dissolved Reactive Phosphorus\nmodel_drp, scaler_drp, results_drp = run_pipeline(X, y_drp, param_name=\"Dissolved Reactive Phosphorus\")\n\n# Combine all results\nprint(\"\\n\" + \"=\" * 80)\nprint(\"üìä FINAL MODEL COMPARISON\")\nprint(\"=\" * 80)\n\nall_results = pd.concat([results_ta, results_ec, results_drp], ignore_index=True)\ndisplay(all_results)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ PIPELINE COMPLETE - ALL MODELS TRAINED AND EVALUATED!\")\nprint(\"=\" * 80)\n\n# Summary insights\nprint(\"\\nüìà Model Performance Insights:\")\nfor idx, row in all_results.iterrows():\n    print(f\"\\n{row['Parameter']}:\")\n    print(f\"  Training R¬≤: {row['R2_Train']:.3f}\")\n    print(f\"  Test R¬≤: {row['R2_Test']:.3f}\")\n    \n    # Check for overfitting\n    if row['R2_Train'] - row['R2_Test'] > 0.1:\n        print(f\"  ‚ö†Ô∏è  Possible overfitting detected (Train-Test gap: {row['R2_Train'] - row['R2_Test']:.3f})\")\n    else:\n        print(f\"  ‚úÖ Model generalizes well\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1783a30e-ee66-4c50-a957-b67296e9328c",
   "metadata": {
    "codeCollapsed": true,
    "name": "cell34",
    "collapsed": false
   },
   "source": [
    "### Model Training and Evaluation for Each Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5f39fd-a577-4eff-9fa5-caa9d7f2fcda",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell35"
   },
   "source": [
    "In this step, we apply the complete modeling pipeline to each of the three selected water quality parameters ‚Äî Total Alkalinity, Electrical Conductance, and Dissolved Reactive Phosphorus. The input feature set (`X`) remains the same across all three models, while the target variable (`y`) changes for each parameter. \n",
    "\n",
    "For every parameter, the `run_pipeline()` function is executed, which handles data preprocessing, model training, and both in-sample and out-of-sample evaluation. This ensures a consistent workflow and allows for a fair comparison of model performance across different water quality indicators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2d5f62b-585c-45e0-8fa9-73a5fa1248f8",
   "metadata": {
    "language": "python",
    "name": "cell36"
   },
   "outputs": [],
   "source": "# ============================================\n# Run Pipeline for All Three Water Quality Parameters\n# ============================================\nprint(\"=\" * 80)\nprint(\"üöÄ RUNNING PIPELINE FOR ALL WATER QUALITY PARAMETERS\")\nprint(\"=\" * 80)\n\nprint(\"\\nIn this step, we apply the complete modeling pipeline to each of the three\")\nprint(\"selected water quality parameters ‚Äî Total Alkalinity, Electrical Conductance,\")\nprint(\"and Dissolved Reactive Phosphorus. The input feature set (X) remains the same\")\nprint(\"across all three models, while the target variable (y) changes for each parameter.\")\n\nprint(\"\\nFor every parameter, the run_pipeline() function is executed, which handles\")\nprint(\"data preprocessing, model training, and both in-sample and out-of-sample\")\nprint(\"evaluation. This ensures a consistent workflow and allows for a fair comparison\")\nprint(\"of model performance across different water quality indicators.\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# First, check what columns we actually have\nprint(\"\\nüîç Available columns in wq_data:\")\nprint(wq_data.columns.tolist())\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Define feature set (X) - use correct column names (UPPERCASE with underscores)\nX = wq_data[['SWIR22', 'NDMI', 'MNDWI', 'PET']]\n\n# Define target variables (y) for each parameter - use correct column names\ny_TA = wq_data['TOTAL_ALKALINITY']\ny_EC = wq_data['ELECTRICAL_CONDUCTANCE']\ny_DRP = wq_data['DISSOLVED_REACTIVE_PHOSPHORUS']\n\nprint(\"\\nüéØ Feature set (X) shape:\", X.shape)\nprint(\"   Features:\", X.columns.tolist())\n\nprint(\"\\nüéØ Target variables:\")\nprint(f\"   y_TA (Total Alkalinity): {y_TA.shape[0]} samples\")\nprint(f\"   y_EC (Electrical Conductance): {y_EC.shape[0]} samples\")\nprint(f\"   y_DRP (Dissolved Reactive Phosphorus): {y_DRP.shape[0]} samples\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Run pipeline for Total Alkalinity\nprint(\"\\nüìä PARAMETER 1: TOTAL ALKALINITY\")\nmodel_TA, scaler_TA, results_TA = run_pipeline(X, y_TA, \"Total Alkalinity\")\n\n# Run pipeline for Electrical Conductance\nprint(\"\\nüìä PARAMETER 2: ELECTRICAL CONDUCTANCE\")\nmodel_EC, scaler_EC, results_EC = run_pipeline(X, y_EC, \"Electrical Conductance\")\n\n# Run pipeline for Dissolved Reactive Phosphorus\nprint(\"\\nüìä PARAMETER 3: DISSOLVED REACTIVE PHOSPHORUS\")\nmodel_DRP, scaler_DRP, results_DRP = run_pipeline(X, y_DRP, \"Dissolved Reactive Phosphorus\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ ALL PIPELINES EXECUTED SUCCESSFULLY!\")\nprint(\"=\" * 80)\n\n# Combine all results into one DataFrame\nall_results = pd.concat([results_TA, results_EC, results_DRP], ignore_index=True)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"üìä FINAL MODEL PERFORMANCE SUMMARY\")\nprint(\"=\" * 80)\ndisplay(all_results)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"üìà MODEL PERFORMANCE ANALYSIS\")\nprint(\"=\" * 80)\n\nfor idx, row in all_results.iterrows():\n    print(f\"\\n{row['Parameter']}:\")\n    print(f\"  ‚úì Training R¬≤: {row['R2_Train']:.3f} | RMSE: {row['RMSE_Train']:.3f}\")\n    print(f\"  ‚úì Test R¬≤: {row['R2_Test']:.3f} | RMSE: {row['RMSE_Test']:.3f}\")\n    \n    # Check for overfitting\n    r2_gap = row['R2_Train'] - row['R2_Test']\n    if r2_gap > 0.1:\n        print(f\"  ‚ö†Ô∏è  Warning: Possible overfitting (R¬≤ gap: {r2_gap:.3f})\")\n    else:\n        print(f\"  ‚úÖ Model generalizes well (R¬≤ gap: {r2_gap:.3f})\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚ú® MODEL TRAINING AND EVALUATION COMPLETE!\")\nprint(\"=\" * 80)\n\n# Store models and scalers for later use\nprint(\"\\nüíæ Trained models and scalers saved:\")\nprint(\"   ‚Ä¢ model_TA, scaler_TA - Total Alkalinity\")\nprint(\"   ‚Ä¢ model_EC, scaler_EC - Electrical Conductance\")\nprint(\"   ‚Ä¢ model_DRP, scaler_DRP - Dissolved Reactive Phosphorus\")"
  },
  {
   "cell_type": "markdown",
   "id": "644a854c-a8af-411e-b5af-e78d064d80a1",
   "metadata": {
    "codeCollapsed": true,
    "name": "cell37",
    "collapsed": false
   },
   "source": [
    "### Model Performance Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b4c7a8-c192-469f-8a18-972f4900da2b",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell38"
   },
   "source": [
    "After training and evaluating the models for each water quality parameter, the individual performance metrics are combined into a single summary table. This table consolidates the R¬≤ and RMSE values for both in-sample and out-of-sample evaluations, enabling an easy comparison of model performance across Total Alkalinity, Electrical Conductance, and Dissolved Reactive Phosphorus. \n",
    "\n",
    "Such a summary provides a quick overview of how well each model captures the variability in each parameter and highlights any differences in predictive accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7f3a047-a05c-4369-8a84-5b287f1ff272",
   "metadata": {
    "language": "python",
    "name": "cell39"
   },
   "outputs": [],
   "source": "# ============================================\n# IMPROVED MODEL TRAINING WITH MORE FEATURES\n# ============================================\nprint(\"=\" * 80)\nprint(\"üöÄ IMPROVED PIPELINE WITH ANTI-OVERFITTING MEASURES\")\nprint(\"=\" * 80)\n\nprint(\"\\nüí° Improvements Applied:\")\nprint(\"  1. Using MORE features (not just 4)\")\nprint(\"  2. Better Random Forest hyperparameters to reduce overfitting\")\nprint(\"  3. Regularization through max_depth and min_samples constraints\")\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Step 1: Collect ALL available features\navailable_features = []\n\n# Must-have features\nmust_have = ['SWIR22', 'NDMI', 'MNDWI', 'PET']\nfor feat in must_have:\n    if feat in wq_data.columns:\n        available_features.append(feat)\n\n# Optional additional features (spectral bands)\noptional = ['NIR', 'GREEN', 'RED', 'BLUE', 'SWIR16', 'SWIR1', 'SWIR2']\nfor feat in optional:\n    if feat in wq_data.columns and feat not in available_features:\n        available_features.append(feat)\n\nprint(f\"\\nüéØ Using {len(available_features)} features:\")\nfor i, feat in enumerate(available_features, 1):\n    print(f\"  {i}. {feat}\")\n\n# Create feature matrix\nX = wq_data[available_features]\n\n# Define targets\ny_TA = wq_data['TOTAL_ALKALINITY']\ny_EC = wq_data['ELECTRICAL_CONDUCTANCE']\ny_DRP = wq_data['DISSOLVED_REACTIVE_PHOSPHORUS']\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Define improved training function\ndef train_model_improved(X_train_scaled, y_train):\n    \"\"\"Random Forest with anti-overfitting settings\"\"\"\n    model = RandomForestRegressor(\n        n_estimators=100,\n        max_depth=10,\n        min_samples_split=20,\n        min_samples_leaf=10,\n        max_features='sqrt',\n        random_state=42,\n        n_jobs=-1\n    )\n    model.fit(X_train_scaled, y_train)\n    return model\n\n# Define improved pipeline\ndef run_pipeline_improved(X, y, param_name=\"Parameter\"):\n    \"\"\"Complete ML pipeline with improved model\"\"\"\n    print(f\"\\n{'=' * 60}\")\n    print(f\"Training Improved Model for {param_name}\")\n    print(f\"{'=' * 60}\")\n    \n    # Split\n    X_train, X_test, y_train, y_test = split_data(X, y)\n    print(f\"  Train: {X_train.shape[0]} samples | Test: {X_test.shape[0]} samples\")\n    \n    # Scale\n    X_train_scaled, X_test_scaled, scaler = scale_data(X_train, X_test)\n    \n    # Train with improved settings\n    model = train_model_improved(X_train_scaled, y_train)\n    \n    # Evaluate\n    y_train_pred, r2_train, rmse_train = evaluate_model(model, X_train_scaled, y_train, \"Train\")\n    y_test_pred, r2_test, rmse_test = evaluate_model(model, X_test_scaled, y_test, \"Test\")\n    \n    # Return results\n    results = {\n        \"Parameter\": param_name,\n        \"R2_Train\": r2_train,\n        \"RMSE_Train\": rmse_train,\n        \"R2_Test\": r2_test,\n        \"RMSE_Test\": rmse_test\n    }\n    \n    return model, scaler, pd.DataFrame([results])\n\n# Run improved pipeline for all parameters\nprint(\"\\nüìä TRAINING IMPROVED MODELS...\")\nprint(\"=\" * 80)\n\nmodel_TA_v2, scaler_TA_v2, results_TA_v2 = run_pipeline_improved(X, y_TA, \"Total Alkalinity\")\nmodel_EC_v2, scaler_EC_v2, results_EC_v2 = run_pipeline_improved(X, y_EC, \"Electrical Conductance\")\nmodel_DRP_v2, scaler_DRP_v2, results_DRP_v2 = run_pipeline_improved(X, y_DRP, \"Dissolved Reactive Phosphorus\")\n\n# Combine results\nresults_summary_improved = pd.concat([results_TA_v2, results_EC_v2, results_DRP_v2], ignore_index=True)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"üìä IMPROVED MODEL PERFORMANCE\")\nprint(\"=\" * 80)\ndisplay(results_summary_improved)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"üìà COMPARISON: OLD vs NEW\")\nprint(\"=\" * 80)\n\ncomparison = pd.DataFrame({\n    'Parameter': results_summary['Parameter'],\n    'Old_R2_Test': results_summary['R2_Test'],\n    'New_R2_Test': results_summary_improved['R2_Test'],\n    'Improvement': results_summary_improved['R2_Test'] - results_summary['R2_Test'],\n    'Old_Gap': results_summary['R2_Train'] - results_summary['R2_Test'],\n    'New_Gap': results_summary_improved['R2_Train'] - results_summary_improved['R2_Test']\n})\n\ndisplay(comparison)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚ú® IMPROVEMENTS APPLIED SUCCESSFULLY!\")\nprint(\"=\" * 80)\n\nfor idx, row in comparison.iterrows():\n    print(f\"\\n{row['Parameter']}:\")\n    print(f\"  Old R¬≤ Test: {row['Old_R2_Test']:.4f} | New R¬≤ Test: {row['New_R2_Test']:.4f}\")\n    print(f\"  Improvement: {row['Improvement']:+.4f}\")\n    print(f\"  Overfitting Gap: {row['Old_Gap']:.4f} ‚Üí {row['New_Gap']:.4f}\")\n    \n    if row['New_Gap'] < row['Old_Gap']:\n        print(f\"  ‚úÖ Overfitting REDUCED by {(row['Old_Gap'] - row['New_Gap']):.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "b5277b08-d7bc-4b7c-91bf-26fa19795f56",
   "metadata": {
    "codeCollapsed": true,
    "name": "cell41"
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c5dde-8a39-4e16-bdd9-93ce024850d2",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell42"
   },
   "source": [
    "Once you are satisfied with your model‚Äôs performance, you can proceed to make predictions for unseen data. To do this, use your trained model to estimate the concentrations of the target water quality parameters ‚Äî Total Alkalinity, Electrical Conductance, and Dissolved Reactive Phosphorus ‚Äî for a set of test locations provided in the **Submission_template.csv** file. \n",
    "\n",
    "The predicted results can then be uploaded to the challenge platform for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1475b7c2-6562-4274-9bd6-98b305010351",
   "metadata": {
    "language": "python",
    "name": "cell40"
   },
   "outputs": [],
   "source": [
    "test_file = pd.read_csv(\"submission_template.csv\")\n",
    "display(test_file.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e85fe2-7570-432e-b1f2-bc16dda08ab4",
   "metadata": {
    "language": "python",
    "name": "cell43"
   },
   "outputs": [],
   "source": [
    "landsat_val_features = pd.read_csv(\"landsat_features_validation.csv\")\n",
    "display(landsat_val_features.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e53ead-e1b6-4d7d-93bb-12505fbb2ead",
   "metadata": {
    "language": "python",
    "name": "cell45"
   },
   "outputs": [],
   "source": [
    "Terraclimate_val_df = pd.read_csv(\"terraclimate_features_validation.csv\")\n",
    "display(Terraclimate_val_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feacaed-272c-4812-8c14-d2bd6a8d48f1",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell44"
   },
   "source": [
    "Similarly, participants can use the **Landsat** and **TerraClimate** data extraction demonstration notebooks to produce feature CSVs for their **validation** data. For convenience, we have already computed and saved example validation outputs as `landsat_features_val_V3.csv` and `Terraclimate_val_df_v3.csv`. \n",
    "\n",
    "Participants should save their own extracted files in the same format and column schema; doing so will allow this benchmark notebook to load the validation features directly and run smoothly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1dabb48-7f82-4acf-b509-21cc15a5a4e0",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell47"
   },
   "outputs": [],
   "source": [
    "#Consolidate all the extracted bands and features in a single dataframe\n",
    "val_data = pd.DataFrame({\n",
    "    'Longitude': landsat_val_features['Longitude'].values,\n",
    "    'Latitude': landsat_val_features['Latitude'].values,\n",
    "    'Sample Date': landsat_val_features['Sample Date'].values,\n",
    "    'nir': landsat_val_features['nir'].values,\n",
    "    'green': landsat_val_features['green'].values,\n",
    "    'swir16': landsat_val_features['swir16'].values,\n",
    "    'swir22': landsat_val_features['swir22'].values,\n",
    "    'NDMI': landsat_val_features['NDMI'].values,\n",
    "    'MNDWI': landsat_val_features['MNDWI'].values,\n",
    "    'pet': Terraclimate_val_df['pet'].values,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c41d268-0657-490b-94f3-8ffeb67c2265",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell48"
   },
   "outputs": [],
   "source": [
    "# Impute the missing values\n",
    "val_data = val_data.fillna(val_data.median(numeric_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3126bf77-fb54-4609-a09c-8e36b496d108",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell49"
   },
   "outputs": [],
   "source": [
    "# Extracting specific columns (swir22, NDMI, MNDWI, pet) from the validation dataset\n",
    "submission_val_data=val_data.loc[:,['swir22','NDMI','MNDWI','pet']]\n",
    "display(submission_val_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7b033c7-3d5f-4179-b8f8-182f8caad0dc",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell50"
   },
   "outputs": [],
   "source": [
    "submission_val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87b59132-ff14-421e-b37b-472a0adcd9da",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell51"
   },
   "outputs": [],
   "source": [
    "# --- Predicting for Total Alkalinity ---\n",
    "X_sub_scaled_TA = scaler_TA.transform(submission_val_data)\n",
    "pred_TA_submission = model_TA.predict(X_sub_scaled_TA)\n",
    "\n",
    "# --- Predicting for Electrical Conductance ---\n",
    "X_sub_scaled_EC = scaler_EC.transform(submission_val_data)\n",
    "pred_EC_submission = model_EC.predict(X_sub_scaled_EC)\n",
    "\n",
    "# --- Predicting for Dissolved Reactive Phosphorus ---\n",
    "X_sub_scaled_DRP = scaler_DRP.transform(submission_val_data)\n",
    "pred_DRP_submission = model_DRP.predict(X_sub_scaled_DRP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "398e375c-60cc-4fa5-a697-ae12fdab300c",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell52"
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    'Longitude': test_file['Longitude'].values,\n",
    "    'Latitude': test_file['Latitude'].values,\n",
    "    'Sample Date': test_file['Sample Date'].values,\n",
    "    'Total Alkalinity': pred_TA_submission,\n",
    "    'Electrical Conductance': pred_EC_submission,\n",
    "    'Dissolved Reactive Phosphorus': pred_DRP_submission\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46e1c4fa-e49f-40cf-ac10-729b82c4b37b",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell53"
   },
   "outputs": [],
   "source": [
    "#Displaying the sample submission dataframe\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "id": "81909a89-1d3f-4aef-81a3-2db3f241721b",
   "metadata": {
    "language": "sql",
    "name": "cell56"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d3a8c41-dba2-43e4-b84c-a50a096980e7",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell54"
   },
   "outputs": [],
   "source": [
    "#Dumping the predictions into a csv file.\n",
    "submission_df.to_csv(\"/tmp/submission.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2ba55-0dd6-43ab-902d-6adbfdfaca2c",
   "metadata": {
    "language": "python",
    "name": "cell46"
   },
   "outputs": [],
   "source": [
    "session.sql(f\"\"\"\n",
    "    PUT file:///tmp/submission.csv\n",
    "    snow://workspace/USER$.PUBLIC.DEFAULT$/versions/live/\n",
    "    AUTO_COMPRESS=FALSE\n",
    "    OVERWRITE=TRUE\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"File saved! Refresh the browser to see the files in the sidebar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7af545-7a55-4b59-9edc-bc43f47bffd5",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell55"
   },
   "source": [
    "### Upload submission file on platform\n",
    "\n",
    "Upload the `submission.csv` file on the challenge platform to generate your score on the leaderboard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab09ba-3b69-4a2a-a56e-2dbf084e0c56",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "cell57"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Now that you have learned a basic approach to model training, it‚Äôs time to explore your own techniques and ideas! Feel free to modify any of the functions presented in this notebook to experiment with alternative preprocessing steps, feature engineering strategies, or machine learning algorithms. \n",
    "\n",
    "We look forward to seeing your enhanced model and the insights you uncover. Best of luck with the challenge!\n"
   ]
  }
 ]
}